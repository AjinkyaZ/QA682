{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ajinkya/.local/lib/python3.5/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import json\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from random import random\n",
    "import numpy as np\n",
    "import _pickle as pkl\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/train-v1.1.json', 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "with open('../data/dev-v1.1.json', 'r') as f:\n",
    "    test_data = json.load(f) # use as test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 442/442 [00:00<00:00, 720.86it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 267.28it/s]\n"
     ]
    }
   ],
   "source": [
    "data_struct = {'train_X': [], 'train_y': [], 'test_X': [], 'test_y': []} # structured data\n",
    "split_ratio = 0.2\n",
    "data_tokens = []\n",
    "\n",
    "for dataset in ['train', 'test']:\n",
    "    if dataset == 'train':\n",
    "        data = train_data\n",
    "    else:\n",
    "        data = test_data\n",
    "    for d in tqdm(data['data']):\n",
    "        paras = d['paragraphs']\n",
    "        for para in paras:\n",
    "            context = para['context']\n",
    "            data_tokens.extend(context.lower().strip().split(\" \"))\n",
    "            qas = para['qas']\n",
    "            for qa in qas:\n",
    "                q_text = qa['question']\n",
    "                data_tokens.extend(q_text.lower().strip().split(\" \"))\n",
    "                for a in qa['answers']:\n",
    "                    start, ans_text = a['answer_start'], a['text']\n",
    "                    X_q = (context, q_text, ans_text)\n",
    "                    ans = (start, start+len(ans_text))\n",
    "                    data_tokens.extend(ans_text.lower().strip().split(\" \"))\n",
    "                    data_struct[dataset+'_X'].append(X_q)\n",
    "                    data_struct[dataset+'_y'].append(ans)\n",
    "\n",
    "def clean(token):\n",
    "    cleaned_token = token.strip(\".,?!-:;'()[]\\\"`\")\n",
    "    if cleaned_token[-2:] == \"'s\":\n",
    "        cleaned_token = cleaned_token[:-2]\n",
    "    if cleaned_token[-2:] == \"'t\":\n",
    "        cleaned_token = cleaned_token[:-2]+'t'\n",
    "    return cleaned_token\n",
    "\n",
    "data_tokens = [clean(i.lower()) for i in data_tokens]\n",
    "data_tokens = [i for i in data_tokens if len(i)>0]\n",
    "data_vocab = Counter(data_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_test 34726\n",
      "X_val 21900\n",
      "y_test 34726\n",
      "X_train 65699\n",
      "y_train 65699\n",
      "y_val 21900\n"
     ]
    }
   ],
   "source": [
    "raw_data = {'X_train': None,\n",
    "        'y_train': None,\n",
    "        'X_val': None,\n",
    "        'y_val': None,\n",
    "        'X_test': data_struct['test_X'],\n",
    "        'y_test': data_struct['test_y']}\n",
    "raw_data['X_train'], raw_data['X_val'], raw_data['y_train'], raw_data['y_val'] \\\n",
    "     = train_test_split(data_struct['train_X'], data_struct['train_y'], test_size=0.25)\n",
    "for k, v in raw_data.items():\n",
    "    print(k, len(v))\n",
    "with open('../data/data.json', 'w') as f:\n",
    "    json.dump(raw_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/glove/glove.6B.50d.pkl', 'rb') as f:\n",
    "    glove_dict = pkl.load(f)\n",
    "print(len(glove_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "oov = []\n",
    "for w, c in data_vocab.most_common():\n",
    "    if w not in glove_dict:\n",
    "        oov.append((w, c))\n",
    "pprint(sorted(oov, key=lambda x:-x[1])[:10])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
