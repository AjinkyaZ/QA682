{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable as var\n",
    "from torch.nn import functional as F\n",
    "import torchtext.vocab as vocab\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import json\n",
    "import _pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = vocab.GloVe(name='6B', dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(token):\n",
    "    cleaned_token = token.strip(\".,?!-:;'()[]\\\"`\")\n",
    "    if cleaned_token[-2:] == \"'s\":\n",
    "        cleaned_token = cleaned_token[:-2]\n",
    "    if cleaned_token[-2:] == \"'t\":\n",
    "        cleaned_token = cleaned_token[:-2]+'t'\n",
    "    return cleaned_token\n",
    "\n",
    "def get_vector(word):\n",
    "    return glove.vectors[glove.stoi[word]]\n",
    "\n",
    "def vectorize(input_txt, only_idx=False):\n",
    "    input_seq = [clean(w) for w in input_txt if len(clean(w).strip())][:200]\n",
    "    if only_idx:\n",
    "        glove_vec = [glove.stoi[w] for w in input_seq]\n",
    "        return torch.LongTensor(glove_vec)\n",
    "    else:\n",
    "        glove_vec = [get_vector(w).tolist() for w in input_seq]\n",
    "        return torch.FloatTensor(glove_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/data.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: From 1989 through 1996, the total area of the US was listed as 9,372,610 km2 (3,618,780 sq mi) (land + inland water only). The listed total area changed to 9,629,091 km2 (3,717,813 sq mi) in 1997 (Great Lakes area and coastal waters added), to 9,631,418 km2 (3,718,711 sq mi) in 2004, to 9,631,420 km2 (3,718,710 sq mi) in 2006, and to 9,826,630 km2 (3,794,080 sq mi) in 2007 (territorial waters added). Currently, the CIA World Factbook gives 9,826,675 km2 (3,794,100 sq mi), the United Nations Statistics Division gives 9,629,091 km2 (3,717,813 sq mi), and the Encyclopædia Britannica gives 9,522,055 km2 (3,676,486 sq mi)(Great Lakes area included but not coastal waters). These source consider only the 50 states and the Federal District, and exclude overseas territories.\n",
      "Question: According to the Encyclopedia Britannica, what is the total area of the US in miles?\n",
      "Answer Span: [608, 623]\n",
      "Answer: 3,676,486 sq mi\n"
     ]
    }
   ],
   "source": [
    "idx = 2\n",
    "example_X = (data['X_train'][idx])\n",
    "example_y = (data['y_train'][idx])\n",
    "print(\"Context:\", example_X[0])\n",
    "print(\"Question:\", example_X[1])\n",
    "print(\"Answer Span:\", example_y)\n",
    "print(\"Answer:\", example_X[0][example_y[0]:example_y[1]])\n",
    "X = vectorize(example_X[0].lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(raw_X):\n",
    "    X = []\n",
    "    for (c, q, _) in raw_X:\n",
    "        context_rep = vectorize(c.lower(), only_idx=True)\n",
    "        ques_rep = vectorize(q.lower(), only_idx=True)\n",
    "        X.append(context_rep) #only context for now\n",
    "    return X\n",
    "X_pass = make_data(data['X_train'][:10])\n",
    "y_pass = data['y_train'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelV1(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, batch_size, n_layers=1, opt='adam', lr=1e-3, epochs=5, emb_dim=50):\n",
    "        super(ModelV1, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.batch_size = batch_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        if opt == 'adam':\n",
    "            self.opt = optim.Adam\n",
    "        else:\n",
    "            self.opt = optim.SGD\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.emb_dim = emb_dim\n",
    "        \n",
    "        self.encoder = nn.Embedding(len(glove.vectors), self.emb_dim)\n",
    "        self.lstm = nn.LSTM(self.emb_dim, self.hidden_size, self.n_layers)\n",
    "        self.decoder = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        weight_scale = 0.01\n",
    "        self.encoder.weight.data = glove.vectors\n",
    "        self.decoder.bias.data.fill_(0)\n",
    "        self.decoder.weight.data.uniform_(-weight_scale, weight_scale)\n",
    "    \n",
    "    def init_hidden(self, batch_size=1):\n",
    "        weight = next(self.parameters()).data\n",
    "        return var(weight.new(self.n_layers, self.batch_size, self.hidden_size).zero_())\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        inputs = inputs[0]\n",
    "        embeds = self.encoder(var(inputs)) # get glove repr\n",
    "        lstm_op, (hnext, cnext) = self.lstm(embeds, hidden)\n",
    "        last = lstm_op[-1]\n",
    "        out = self.decoder(last)\n",
    "        return out, (hnext, cnext)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        opt = self.opt(self.parameters(), self.lr)\n",
    "        h, c = self.init_hidden(), self.init_hidden()\n",
    "\n",
    "        losses = []\n",
    "        for epoch in tqdm(range(self.epochs)):\n",
    "            bs = 1\n",
    "            tloss = 0.0\n",
    "            for i in range(0, len(y)-bs+1, bs):\n",
    "                opt.zero_grad()\n",
    "                Xb = X[i:i+bs]\n",
    "                yb = var(torch.FloatTensor(y[i:i+bs]))\n",
    "                pred, (h, c) = self.forward(Xb, (h, c)) #prediction on batch features\n",
    "                loss = F.mse_loss(pred, yb)\n",
    "                tloss += loss.data[0]\n",
    "                loss.backward(retain_graph=True)\n",
    "                opt.step()\n",
    "            losses.append(tloss)\n",
    "        self.last_hidden = (h, c)\n",
    "        return losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.forward(X, self.last_hidden)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ModelV1(200, 100, 2, batch_size=1, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:50<00:00, 25.40s/it]\n"
     ]
    }
   ],
   "source": [
    "res = model.fit(X_pass, y_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.9397  1.0401\n",
       "[torch.FloatTensor of size 1x2]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict([X_pass[2]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
