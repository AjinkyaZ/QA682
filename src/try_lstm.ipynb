{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable as var\n",
    "from torch.nn import functional as F\n",
    "import torchtext.vocab as vocab\n",
    "from tqdm import tqdm\n",
    "from pprint import pprint\n",
    "import json\n",
    "import _pickle as pkl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(token):\n",
    "    cleaned_token = token.strip(\".,?!-:;'()[]\\\"`\")\n",
    "    if cleaned_token[-2:] == \"'s\":\n",
    "        cleaned_token = cleaned_token[:-2]\n",
    "    if cleaned_token[-2:] == \"'t\":\n",
    "        cleaned_token = cleaned_token[:-2]+'t'\n",
    "    return cleaned_token\n",
    "\n",
    "glove = vocab.GloVe(name='6B', dim=50)\n",
    "\n",
    "def get_vector(word):\n",
    "    return glove.vectors[glove.stoi[word]]\n",
    "\n",
    "glove.stoi['<unk>'] = len(glove.stoi)+1 # add token->index for unknown/oov\n",
    "glove.vectors = torch.cat((glove.vectors, torch.zeros(1, 50))) # add index->vec for unknown/oov\n",
    "\n",
    "def vectorize(input_txt, max_len, only_idx=True):\n",
    "    input_seq = [clean(w) for w in input_txt if len(clean(w).strip())]\n",
    "    glove_vec = []\n",
    "    if only_idx:\n",
    "        for w in input_seq:\n",
    "            try:\n",
    "                glove_vec.append(glove.stoi[w])\n",
    "            except:\n",
    "                glove_vec.append(400000)\n",
    "        if len(glove_vec)<max_len:\n",
    "            padding_zeros = [400000]*(max_len-len(glove_vec))\n",
    "            glove_vec = padding_zeros + glove_vec\n",
    "        return torch.LongTensor(glove_vec[:max_len])\n",
    "    else: # add padding same as above - TODO\n",
    "        glove_vec = [get_vector(w).tolist() for w in input_seq]\n",
    "        return torch.FloatTensor(glove_vec[:max_len])\n",
    "    \n",
    "def make_data(raw_X):\n",
    "    X = []\n",
    "    y = []\n",
    "    for (c, q, _) in raw_X:\n",
    "        context_rep = vectorize(c.lower(), 600, only_idx=True)\n",
    "        ques_rep = vectorize(q.lower(), 100, only_idx=True)\n",
    "        X.append(torch.cat((context_rep, ques_rep))) #only context for now\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/data.json', 'r') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400001, 50])\n"
     ]
    }
   ],
   "source": [
    "print(glove.vectors.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context: The storage in sensory memory and short-term memory generally has a strictly limited capacity and duration, which means that information is not retained indefinitely. By contrast, long-term memory can store much larger quantities of information for potentially unlimited duration (sometimes a whole life span). Its capacity is immeasurable. For example, given a random seven-digit number we may remember it for only a few seconds before forgetting, suggesting it was stored in our short-term memory. On the other hand, we can remember telephone numbers for many years through repetition; this information is said to be stored in long-term memory.\n",
      "Question: Why can't some memories be held onto forever?\n",
      "Answer Span: [0, 106]\n",
      "Answer: The storage in sensory memory and short-term memory generally has a strictly limited capacity and duration\n"
     ]
    }
   ],
   "source": [
    "idx = 5\n",
    "example_X = (data['X_train'][idx])\n",
    "example_y = (data['y_train'][idx])\n",
    "print(\"Context:\", example_X[0])\n",
    "print(\"Question:\", example_X[1])\n",
    "print(\"Answer Span:\", example_y)\n",
    "print(\"Answer:\", example_X[2])\n",
    "X = make_data([example_X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ex = 10\n",
    "X_pass = make_data(data['X_train'][:num_ex])\n",
    "y_pass = data['y_train'][:num_ex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelV1(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super(ModelV1, self).__init__()\n",
    "        \n",
    "        self.input_size = config.get(\"input_size\", 700)\n",
    "        self.hidden_size = config.get(\"hidden_size\", 128)\n",
    "        self.output_size = config.get(\"output_size\", 5000)\n",
    "        self.n_layers = config.get(\"n_layers\", 1)\n",
    "        self.vocab_size = config.get(\"vocab\", 400001)\n",
    "        self.emb_dim = config.get(\"embedding_dim\", 50)\n",
    "        self.bidir = config.get(\"Bidirectional\", True)\n",
    "        self.dirs = int(self.bidir)+1\n",
    "        self.lr = config.get(\"learning_rate\", 1e-3)\n",
    "        self.batch_size = config.get(\"batch_size\", 1)\n",
    "        self.epochs = config.get(\"epochs\", 2)\n",
    "        self.opt = config.get(\"opt\", \"SGD\")\n",
    "        \n",
    "        if self.opt == 'Adam':\n",
    "            self.opt = optim.Adam\n",
    "        else:\n",
    "            self.opt = optim.SGD\n",
    "        \n",
    "        self.encoder = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "        self.lstm = nn.LSTM(self.emb_dim, self.hidden_size, self.n_layers, bidirectional=self.bidir)\n",
    "        self.decoder_start = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.decoder_end = nn.Linear(self.hidden_size, self.output_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        weight_scale = 0.01\n",
    "        self.encoder.weight.data = glove.vectors\n",
    "        self.decoder_start.bias.data.fill_(0)\n",
    "        self.decoder_start.weight.data.uniform_(-weight_scale, weight_scale)\n",
    "        self.decoder_end.bias.data.fill_(0)\n",
    "        self.decoder_end.weight.data.uniform_(-weight_scale, weight_scale)\n",
    "\n",
    "    def init_hidden(self):\n",
    "        weight = next(self.parameters()).data\n",
    "        return var(weight.new(self.n_layers*self.dirs, self.batch_size, self.hidden_size).zero_())\n",
    "        \n",
    "    def forward(self, inputs, hidden):\n",
    "        inputs = inputs[0]\n",
    "        embeds = self.encoder(var(torch.LongTensor(inputs).unsqueeze(1))) # get glove repr\n",
    "        # print(\"embeds:\", embeds.size())\n",
    "        seq_len = embeds.size()[0]\n",
    "        lstm_op, (hnext, cnext) = self.lstm(embeds, hidden)\n",
    "        # print(\"lstm op:\", lstm_op.size()) # (seq_len, bs, hidden_size*(dirs=2 for bi))\n",
    "        lstm_op = lstm_op.permute(1, 0, 2) # (seq_len, bs, hdim)->(bs, seq_len, hdim)\n",
    "        start_pred = lstm_op[:, 0, :self.hidden_size] # reverse direction\n",
    "        end_pred = lstm_op[:, -1, self.hidden_size:] # forward direction\n",
    "        # print(\"lstm start, end preds:\", start_pred.size(), end_pred.size())\n",
    "        out_start = F.log_softmax(F.relu(self.decoder_start(start_pred)), dim=-1)\n",
    "        out_end = F.log_softmax(F.relu(self.decoder_end(end_pred)), dim=-1)\n",
    "        # print(\"outs:\", out_start.size(), out_end.size())\n",
    "        return (out_start, out_end)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        opt = self.opt(self.parameters(), self.lr)\n",
    "        losses = [] # epoch loss\n",
    "        for epoch in range(self.epochs):\n",
    "            print(\"epoch:\", epoch, end=', loss: ')\n",
    "            bs = 1\n",
    "            bloss = 0.0 # batch loss\n",
    "            for i in range(0, len(y)-bs+1, bs):\n",
    "                h, c = self.init_hidden(), self.init_hidden()\n",
    "                # print(h.size(), c.size())\n",
    "                opt.zero_grad()\n",
    "                Xb = X[i:i+bs]\n",
    "                yb = var(torch.LongTensor(y[i:i+bs]))\n",
    "                pred = self.forward(Xb, (h, c)) #prediction on batch features\n",
    "                # print(pred[0].view(1, -1).size(), yb[0, :, :].size())\n",
    "                loss = F.nll_loss(pred[0].view(1, -1), yb[:, 0]) \\\n",
    "                     + F.nll_loss(pred[1].view(1, -1), yb[:, 1])\n",
    "                bloss += loss.data[0]\n",
    "                # print(bloss)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "            losses.append(bloss)\n",
    "            print(losses[-1], end=', change: ')\n",
    "            if len(losses)>1:\n",
    "                diff = losses[-2]-losses[-1]\n",
    "                rel_diff = diff/losses[-2]\n",
    "                print(\"%s\"%rel_diff, \"%\")\n",
    "            else:\n",
    "                print(\"00.0%\")\n",
    "        return losses\n",
    "\n",
    "    def predict(self, X):\n",
    "        h, c = self.init_hidden(), self.init_hidden()\n",
    "        s_pred, e_pred = self.forward(X, (h, c))\n",
    "        _,  s_index = torch.max(s_pred, -1)\n",
    "        _,  e_index = torch.max(e_pred, -1)\n",
    "        return torch.cat((s_index, e_index), -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = {\"learning_rate\": 0.5, \n",
    "        \"epochs\": 5,\n",
    "        \"hidden_size\": 2}\n",
    "model = ModelV1(conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, loss: "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-ef1dee56a20c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_pass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pass\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-57855e81771b>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mbloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m                 \u001b[0;31m# print(bloss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m                 \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m             \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/variable.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mVariable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \"\"\"\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(variables, grad_variables, retain_graph, create_graph, retain_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m---> 99\u001b[0;31m         variables, grad_variables, retain_graph)\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = model.fit(X_pass, y_pass)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in zip(data['X_train'][60:70], data['y_train'][60:70]):\n",
    "    c = x[0]\n",
    "    a = x[2]\n",
    "    x = make_data([x])\n",
    "    res = model.predict(x).data.tolist()\n",
    "    print(\"Predicted span:\", res)\n",
    "    print(\"Predicted Answer:\", c[res[0]:res[1]])\n",
    "    print(\"Actual:\", a)\n",
    "    print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(list(range(len(res))), res)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
